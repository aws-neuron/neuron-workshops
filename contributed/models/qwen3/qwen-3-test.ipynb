{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall transformers --y\n",
    "!pip install transformers==4.51.3\n",
    "\n",
    "# Installing collected packages: transformers\n",
    "# ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
    "# neuronx-distributed-inference 0.3.5591+f50feae2 requires transformers==4.48.*, but you have transformers 4.52.4 which is incompatible.\n",
    "# Successfully installed transformers-4.52.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "libneuronxla                  2.2.3493.0+78c3e78c\n",
      "neuronx-cc                    2.18.121.0+9e31e41a\n",
      "neuronx-distributed           0.12.12111+cdd84048\n",
      "neuronx-distributed-inference 0.3.5591+f50feae2\n",
      "torch-neuronx                 2.6.0.2.7.5413+113e6810\n",
      "transformers                  4.51.3\n"
     ]
    }
   ],
   "source": [
    "!pip list | grep neuron\n",
    "!pip list | grep transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, GenerationConfig\n",
    "from neuronx_distributed_inference.models.config import NeuronConfig, OnDeviceSamplingConfig\n",
    "from neuronx_distributed_inference.utils.hf_adapter import HuggingFaceGenerationAdapter, load_pretrained_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/home/ubuntu/model_hf_qwen/qwen/\"\n",
    "traced_model_path = \"/home/ubuntu/traced_model_qwen3/qwen3/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "snapshot_download(\"Qwen/Qwen3-8B\", local_dir=model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You may ignore the error that nxdi is not compatible with transformers > 4.48"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/build-on-trainium-workshop/contributed/models/qwen3/modeling_qwen3.py:61: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from neuronx_distributed_inference.modules.attention.attention_base import NeuronAttentionBase\n",
      "/home/ubuntu/build-on-trainium-workshop/contributed/models/qwen3/modeling_qwen3.py:61: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from neuronx_distributed_inference.modules.attention.attention_base import NeuronAttentionBase\n"
     ]
    }
   ],
   "source": [
    "from modeling_qwen3 import Qwen3InferenceConfig, NeuronQwen3ForCausalLM\n",
    "\n",
    "def run_qwen3_compile():\n",
    "    # Initialize configs and tokenizer.\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path, padding_side=\"right\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    generation_config = GenerationConfig.from_pretrained(model_path)\n",
    "    generation_config_kwargs = {\n",
    "        \"do_sample\": True,\n",
    "        \"top_k\": 1,\n",
    "        \"pad_token_id\": tokenizer.pad_token_id,\n",
    "    }\n",
    "    generation_config.update(**generation_config_kwargs)\n",
    " \n",
    "    neuron_config = NeuronConfig(\n",
    "        tp_degree=8,\n",
    "        batch_size=1,\n",
    "        max_context_length=1024, \n",
    "        seq_len=2048, \n",
    "        on_device_sampling_config=OnDeviceSamplingConfig(top_k=5),\n",
    "        enable_bucketing=True,\n",
    "        context_encoding_buckets=[1024],\n",
    "        token_generation_buckets=[2048],\n",
    "        flash_decoding_enabled=False,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        fused_qkv=False,\n",
    "        attn_kernel_enabled=True,\n",
    "        attn_cls=\"NeuronQwen3Attention\"\n",
    "    )\n",
    "    config = Qwen3InferenceConfig(\n",
    "        neuron_config,\n",
    "        load_config=load_pretrained_config(model_path),\n",
    "    )\n",
    "    \n",
    "    # Compile and save model.\n",
    "    print(\"\\nCompiling and saving model...\")\n",
    "    model = NeuronQwen3ForCausalLM(model_path, config)\n",
    "    model.compile(traced_model_path)\n",
    "    tokenizer.save_pretrained(traced_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_qwen3_compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modeling_qwen3 import Qwen3InferenceConfig, NeuronQwen3ForCausalLM\n",
    "\n",
    "model = NeuronQwen3ForCausalLM(traced_model_path)\n",
    "model.load(traced_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neuronx_distributed_inference.models.config.NeuronConfig"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = model.get_config_cls()\n",
    "config.get_neuron_config_cls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.num_attention_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.num_key_value_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4096"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(traced_model_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "generation_config = GenerationConfig.from_pretrained(model_path)\n",
    "generation_config_kwargs = {\n",
    "    \"do_sample\": False,\n",
    "    \"temperature\": 0.9,\n",
    "    \"top_k\": 5,\n",
    "    \"pad_token_id\": tokenizer.pad_token_id,\n",
    "}\n",
    "generation_config.update(**generation_config_kwargs)\n",
    "generation_model = HuggingFaceGenerationAdapter(model)\n",
    "messages = [{'role': 'user', 'content': \"What's your name?\"}]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=False # Switches between thinking and non-thinking modes. Default is True.\n",
    ")\n",
    "inputs = tokenizer([text], return_tensors=\"pt\")\n",
    "input_ids = inputs['input_ids']  \n",
    "\n",
    "outputs = generation_model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_new_tokens=512\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thinking content: \n",
      "content: My name is Qwen, and I'm a large language model developed by Alibaba Cloud. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "output_ids = outputs[0][len(inputs.input_ids[0]):].tolist() \n",
    "\n",
    "# parsing thinking content\n",
    "try:\n",
    "    # rindex finding 151668 (</think>)\n",
    "    index = len(output_ids) - output_ids[::-1].index(151668)\n",
    "except ValueError:\n",
    "    index = 0\n",
    "\n",
    "thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
    "content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
    "\n",
    "print(\"thinking content:\", thinking_content)\n",
    "print(\"content:\", content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thinking example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(traced_model_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "generation_config = GenerationConfig.from_pretrained(model_path)\n",
    "generation_config_kwargs = {\n",
    "    \"do_sample\": False,\n",
    "    \"temperature\": 0.9,\n",
    "    \"top_k\": 5,\n",
    "    \"pad_token_id\": tokenizer.pad_token_id,\n",
    "}\n",
    "generation_config.update(**generation_config_kwargs)\n",
    "generation_model = HuggingFaceGenerationAdapter(model)\n",
    "messages = [{'role': 'system', 'content': \"Only think through one example before providing the correct answer\"},\n",
    "             {'role': 'user', 'content': \"What is 83 * 110 + 34?\"}\n",
    "        ]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=True # Switches between thinking and non-thinking modes. Default is True.\n",
    ")\n",
    "inputs = tokenizer([text], return_tensors=\"pt\")\n",
    "input_ids = inputs['input_ids'] \n",
    "outputs = generation_model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_new_tokens=1024\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thinking content: <think>\n",
      "Okay, let's see. I need to calculate 83 multiplied by 110 and then add 34 to the result. Hmm, let me break this down step by step. First, I should handle the multiplication part: 83 times 110. \n",
      "\n",
      "Wait, multiplying by 110 might be easier if I think of it as multiplying by 100 and then adding 10 times the number. Because 110 is 100 + 10. So, 83 times 100 is 8300, and 83 times 10 is 830. Then adding those two together: 8300 + 830. Let me check that. 8300 plus 800 is 9100, and then plus 30 more would be 9130. So, 83 * 110 equals 9130?\n",
      "\n",
      "Wait, let me verify that another way. Maybe using the standard multiplication method. Let's write it out:\n",
      "\n",
      "   83\n",
      "x110\n",
      "------\n",
      "First, multiply 83 by 0 (the units place of 110), which gives 0.\n",
      "Then multiply 83 by 1 (the tens place of 110), which is 83, but since it's in the tens place, it's actually 830.\n",
      "Then multiply 83 by 1 (the hundreds place of 110), which is 83, but since it's in the hundreds place, it's 8300.\n",
      "Adding those together: 0 + 830 + 8300 = 9130. Okay, that matches my previous result. So 83*110 is indeed 9130.\n",
      "\n",
      "Now, the next part is adding 34 to that result. So 9130 + 34. Let me do that. 9130 plus 30 is 9160, and then plus 4 more is 9164. \n",
      "\n",
      "Wait, let me check again. 9130 + 34. Breaking it down: 9130 + 30 = 9160, then 9160 + 4 = 9164. Yes, that seems right. \n",
      "\n",
      "Alternatively, I can add 34 to 9130 directly. 9130 + 34. The units digit: 0 + 4 = 4. The tens digit: 3 + 3 = 6. The hundreds and above remain the same. So 9164. Yep, that's correct.\n",
      "\n",
      "So putting it all together, 83 multiplied by 110 is 9130, and adding 34 gives 9164. I think that's the right answer. Let me just confirm once more with another method. Maybe using distributive property for the entire expression.\n",
      "\n",
      "Original problem: 83*110 + 34. Let's think of 110 as 100 + 10, so 83*(100 + 10) + 34 = 83*100 + 83*10 + 34 = 8300 + 830 + 34. Adding those: 8300 + 830 is 9130, then 9130 + 34 is 9164. Yep, same result. \n",
      "\n",
      "I think that's solid. No mistakes in the steps. So the final answer should be 9164.\n",
      "</think>\n",
      "################################################################################################################################################################################################################################################################################################################################\n",
      "content: To solve $ 83 \\times 110 + 34 $, we break it into two parts:\n",
      "\n",
      "1. **Multiplication**:  \n",
      "   $ 83 \\times 110 $ can be simplified by recognizing that $ 110 = 100 + 10 $.  \n",
      "   $$\n",
      "   83 \\times 110 = 83 \\times (100 + 10) = (83 \\times 100) + (83 \\times 10) = 8300 + 830 = 9130\n",
      "   $$\n",
      "\n",
      "2. **Addition**:  \n",
      "   Add 34 to the result:  \n",
      "   $$\n",
      "   9130 + 34 = 9164\n",
      "   $$\n",
      "\n",
      "**Final Answer:**  \n",
      "$$\n",
      "\\boxed{9164}\n",
      "$$\n"
     ]
    }
   ],
   "source": [
    "output_ids = outputs[0][len(inputs.input_ids[0]):].tolist() \n",
    "\n",
    "# parsing thinking content\n",
    "try:\n",
    "    # rindex finding 151668 (</think>)\n",
    "    index = len(output_ids) - output_ids[::-1].index(151668)\n",
    "except ValueError:\n",
    "    index = 0\n",
    "\n",
    "thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
    "content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
    "\n",
    "print(\"thinking content:\", thinking_content)\n",
    "print('####'*80)\n",
    "print(\"content:\", content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9164"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans = 83*110+34\n",
    "ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = '/opt/aws_neuronx_venv_pytorch_2_6_nxd_inference/lib/python3.10/site-packages/neuronx_distributed_inference/'\n",
    "!cp modeling_qwen3.py {dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Edit the inference_demo.py file to include the following:\n",
    "\n",
    "```python\n",
    "from .modeling_qwen import NeuronQwen3ForCausalLM\n",
    "\n",
    "MODEL_TYPES = {\n",
    "    \"llama\": {\"causal-lm\": NeuronLlamaForCausalLM},\n",
    "    \"mixtral\": {\"causal-lm\": NeuronMixtralForCausalLM},\n",
    "    \"dbrx\": {\"causal-lm\": NeuronDbrxForCausalLM},\n",
    "    'qwen3': {\"causal-lm\": NeuronQwen3ForCausalLM}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/aws_neuronx_venv_pytorch_2_6_nxd_inference/lib/python3.10/site-packages/neuronx_distributed/modules/moe/expert_mlps.py:11: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from neuronx_distributed.modules.moe.blockwise import (\n",
      "/opt/aws_neuronx_venv_pytorch_2_6_nxd_inference/lib/python3.10/site-packages/neuronx_distributed/modules/moe/expert_mlps.py:11: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from neuronx_distributed.modules.moe.blockwise import (\n",
      "/opt/aws_neuronx_venv_pytorch_2_6_nxd_inference/lib/python3.10/site-packages/neuronx_distributed/modules/moe/expert_mlps.py:11: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from neuronx_distributed.modules.moe.blockwise import (\n",
      "/opt/aws_neuronx_venv_pytorch_2_6_nxd_inference/lib/python3.10/site-packages/neuronx_distributed_inference/modules/attention/utils.py:14: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from neuronx_distributed_inference.modules.custom_calls import neuron_cumsum\n",
      "/opt/aws_neuronx_venv_pytorch_2_6_nxd_inference/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: Set seed for `privateuseone` device does not take effect, please add API's `_is_in_bad_fork` and `manual_seed_all` to `privateuseone` device module.\n",
      "  return fn(*args, **kwargs)\n",
      "/opt/aws_neuronx_venv_pytorch_2_6_nxd_inference/lib/python3.10/site-packages/neuronx_distributed_inference/modules/lora_serving/lora_model.py:12: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from neuronx_distributed_inference.modules.attention.gqa import GQA, GroupQueryAttention_QKV\n",
      "/opt/aws_neuronx_venv_pytorch_2_6_nxd_inference/lib/python3.10/site-packages/neuronx_distributed_inference/modules/lora_serving/lora_model.py:12: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from neuronx_distributed_inference.modules.attention.gqa import GQA, GroupQueryAttention_QKV\n",
      "/opt/aws_neuronx_venv_pytorch_2_6_nxd_inference/lib/python3.10/site-packages/neuronx_distributed_inference/modules/lora_serving/lora_model.py:12: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from neuronx_distributed_inference.modules.attention.gqa import GQA, GroupQueryAttention_QKV\n",
      "/opt/aws_neuronx_venv_pytorch_2_6_nxd_inference/lib/python3.10/site-packages/neuronx_distributed_inference/models/dbrx/modeling_dbrx.py:38: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from neuronx_distributed_inference.modules.attention.attention_base import NeuronAttentionBase\n",
      "/opt/aws_neuronx_venv_pytorch_2_6_nxd_inference/lib/python3.10/site-packages/neuronx_distributed_inference/models/dbrx/modeling_dbrx.py:38: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from neuronx_distributed_inference.modules.attention.attention_base import NeuronAttentionBase\n",
      "/opt/aws_neuronx_venv_pytorch_2_6_nxd_inference/lib/python3.10/site-packages/neuronx_distributed_inference/inference_demo.py:25: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from neuronx_distributed_inference.models.dbrx.modeling_dbrx import NeuronDbrxForCausalLM\n",
      "/opt/aws_neuronx_venv_pytorch_2_6_nxd_inference/lib/python3.10/site-packages/neuronx_distributed_inference/inference_demo.py:27: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from neuronx_distributed_inference.models.mixtral.modeling_mixtral import NeuronMixtralForCausalLM\n",
      "/opt/aws_neuronx_venv_pytorch_2_6_nxd_inference/lib/python3.10/site-packages/neuronx_distributed_inference/models/mllama/modeling_mllama.py:72: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from .modeling_mllama_vision import NeuronMllamaVisionModel  # noqa: E402\n",
      "/opt/aws_neuronx_venv_pytorch_2_6_nxd_inference/lib/python3.10/site-packages/neuronx_distributed_inference/utils/accuracy.py:29: UserWarning: Intel extension for pytorch not found. For faster CPU references install `intel-extension-for-pytorch`.\n",
      "  warnings.warn(\n",
      "/opt/aws_neuronx_venv_pytorch_2_6_nxd_inference/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: Set seed for `privateuseone` device does not take effect, please add API's `_is_in_bad_fork` and `manual_seed_all` to `privateuseone` device module.\n",
      "  return fn(*args, **kwargs)\n",
      "Loading configs...\n",
      "WARNING:root:NeuronConfig init: Unexpected keyword arguments: {'model_type': 'qwen3', 'task_type': 'causal-lm', 'model_path': '/home/ubuntu/model_hf_qwen/qwen/', 'compiled_model_path': '/home/ubuntu/traced_model_qwen/qwen/logit', 'benchmark': True, 'check_accuracy_mode': <CheckAccuracyMode.LOGIT_MATCHING: 'logit-matching'>, 'divergence_difference_tol': 0.001, 'num_tokens_to_check': 400, 'prompts': ['What is 83 * 110 + 34?'], 'top_k': 1, 'top_p': 1.0, 'temperature': 1.0, 'do_sample': False, 'dynamic': False, 'pad_token_id': 151645, 'on_device_sampling': False, 'enable_torch_dist': False, 'enable_lora': False, 'max_loras': 1, 'max_lora_rank': 16, 'skip_warmup': False, 'skip_compile': False, 'compile_only': False, 'compile_dry_run': False, 'hlo_debug': False}\n",
      "\n",
      "Compiling and saving model...\n",
      "INFO:Neuron:Generating HLOs for the following models: ['context_encoding_model', 'token_generation_model']\n",
      "[2025-06-02 17:05:39.464: I neuronx_distributed/parallel_layers/parallel_state.py:592] > initializing tensor model parallel with size 8\n",
      "[2025-06-02 17:05:39.465: I neuronx_distributed/parallel_layers/parallel_state.py:593] > initializing pipeline model parallel with size 1\n",
      "[2025-06-02 17:05:39.465: I neuronx_distributed/parallel_layers/parallel_state.py:594] > initializing context model parallel with size 1\n",
      "[2025-06-02 17:05:39.465: I neuronx_distributed/parallel_layers/parallel_state.py:595] > initializing data parallel with size 1\n",
      "[2025-06-02 17:05:39.465: I neuronx_distributed/parallel_layers/parallel_state.py:596] > initializing world size to 8\n",
      "[2025-06-02 17:05:39.466: I neuronx_distributed/parallel_layers/parallel_state.py:343] [rank_0_pp-1_tp-1_dp-1_cp-1] Chosen Logic for replica groups ret_logic=<PG_Group_Logic.LOGIC1: (<function ascending_ring_PG_group at 0x7f42e2d28ca0>, 'Ascending Ring PG Group')>\n",
      "[2025-06-02 17:05:39.466: I neuronx_distributed/parallel_layers/parallel_state.py:632] [rank_0_pp-1_tp-1_dp-1_cp-1] tp_groups: replica_groups.tp_groups=[[0, 1, 2, 3, 4, 5, 6, 7]]\n",
      "[2025-06-02 17:05:39.466: I neuronx_distributed/parallel_layers/parallel_state.py:633] [rank_0_pp-1_tp-1_dp-1_cp-1] dp_groups: replica_groups.dp_groups=[[0], [1], [2], [3], [4], [5], [6], [7]]\n",
      "[2025-06-02 17:05:39.466: I neuronx_distributed/parallel_layers/parallel_state.py:634] [rank_0_pp-1_tp-1_dp-1_cp-1] pp_groups: replica_groups.pp_groups=[[0], [1], [2], [3], [4], [5], [6], [7]]\n",
      "[2025-06-02 17:05:39.466: I neuronx_distributed/parallel_layers/parallel_state.py:635] [rank_0_pp-1_tp-1_dp-1_cp-1] cp_groups: replica_groups.cp_groups=[[0], [1], [2], [3], [4], [5], [6], [7]]\n",
      "[2025-06-02 17:05:39.466: I neuronx_distributed/parallel_layers/parallel_state.py:636] [rank_0_pp-1_tp-1_dp-1_cp-1] ep_model_groups: replica_groups.ep_model_groups=[[0], [1], [2], [3], [4], [5], [6], [7]]\n",
      "[2025-06-02 17:05:39.466: I neuronx_distributed/parallel_layers/parallel_state.py:637] [rank_0_pp-1_tp-1_dp-1_cp-1] ep_data_groups: replica_groups.ep_data_groups=[[0], [1], [2], [3], [4], [5], [6], [7]]\n",
      "INFO:Neuron:Generating 1 hlos for key: context_encoding_model\n",
      "INFO:Neuron:Started loading module context_encoding_model\n",
      "INFO:Neuron:Finished loading module context_encoding_model in 0.08188652992248535 seconds\n",
      "INFO:Neuron:generating HLO: context_encoding_model, input example shape = torch.Size([1, 512])\n",
      "/opt/aws_neuronx_venv_pytorch_2_6_nxd_inference/lib/python3.10/site-packages/neuronx_distributed/parallel_layers/layers.py:478: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=False):\n",
      "/opt/aws_neuronx_venv_pytorch_2_6_nxd_inference/lib/python3.10/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:210: UserWarning: Received an input tensor that was unused. Tensor will be ignored. (index=1, shape=torch.Size([1, 512]), dtype=torch.int32)\n",
      "  warnings.warn(\n",
      "/opt/aws_neuronx_venv_pytorch_2_6_nxd_inference/lib/python3.10/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:210: UserWarning: Received an input tensor that was unused. Tensor will be ignored. (index=3, shape=torch.Size([1]), dtype=torch.int32)\n",
      "  warnings.warn(\n",
      "/opt/aws_neuronx_venv_pytorch_2_6_nxd_inference/lib/python3.10/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:210: UserWarning: Received an input tensor that was unused. Tensor will be ignored. (index=4, shape=torch.Size([1, 3]), dtype=torch.float32)\n",
      "  warnings.warn(\n",
      "/opt/aws_neuronx_venv_pytorch_2_6_nxd_inference/lib/python3.10/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:210: UserWarning: Received an input tensor that was unused. Tensor will be ignored. (index=5, shape=torch.Size([1]), dtype=torch.int32)\n",
      "  warnings.warn(\n",
      "/opt/aws_neuronx_venv_pytorch_2_6_nxd_inference/lib/python3.10/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:210: UserWarning: Received an input tensor that was unused. Tensor will be ignored. (index=6, shape=torch.Size([1]), dtype=torch.int32)\n",
      "  warnings.warn(\n",
      "INFO:Neuron:Finished generating HLO for context_encoding_model in 2.901122808456421 seconds, input example shape = torch.Size([1, 512])\n",
      "INFO:Neuron:Generating 1 hlos for key: token_generation_model\n",
      "INFO:Neuron:Started loading module token_generation_model\n",
      "INFO:Neuron:Finished loading module token_generation_model in 0.07949113845825195 seconds\n",
      "INFO:Neuron:generating HLO: token_generation_model, input example shape = torch.Size([1, 1])\n",
      "INFO:Neuron:Finished generating HLO for token_generation_model in 2.800884246826172 seconds, input example shape = torch.Size([1, 1])\n",
      "INFO:Neuron:Generated all HLOs in 5.948296308517456 seconds\n",
      "INFO:Neuron:Starting compilation for the priority HLO\n",
      "INFO:Neuron:'token_generation_model' is the priority model with bucket rank 0\n",
      "/opt/aws_neuronx_venv_pytorch_2_6_nxd_inference/lib/python3.10/site-packages/libneuronxla/neuron_cc_wrapper.py:283: SyntaxWarning: str format compiler_flags is discouraged as its handling involves repeated joining and splitting, which can easily make mistakes if something is quoted or escaped. Use list[str] instead. Refer to documentation of the Python subprocess module for details.\n",
      "  warnings.warn(SyntaxWarning(\n",
      "2025-06-02 17:05:45.000556:  16339  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.18.121.0+9e31e41a/MODULE_ff123d67d8e9ddda72ca+91ef39e9/model.neff\n",
      "INFO:Neuron:Done compilation for the priority HLO in 0.18962407112121582 seconds\n",
      "INFO:Neuron:Updating the hlo module with optimized layout\n",
      "INFO:Neuron:Done optimizing weight layout for all HLOs in 0.15496611595153809 seconds\n",
      "INFO:Neuron:Starting compilation for all HLOs\n",
      "/opt/aws_neuronx_venv_pytorch_2_6_nxd_inference/lib/python3.10/site-packages/libneuronxla/neuron_cc_wrapper.py:245: SyntaxWarning: str format compiler_flags is discouraged as its handling involves repeated joining and splitting, which can easily make mistakes if something is quoted or escaped. Use list[str] instead. Refer to documentation of the Python subprocess module for details.\n",
      "  warnings.warn(SyntaxWarning(\n",
      "2025-06-02 17:05:45.000888:  16339  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.18.121.0+9e31e41a/MODULE_f6025f1aaa134ee9ebd5+d43b5474/model.neff\n",
      "INFO:Neuron:Finished Compilation for all HLOs in 0.13458752632141113 seconds\n",
      "..Completed run_backend_driver.\n",
      "\n",
      "Compiler status PASS\n",
      "INFO:Neuron:Done preparing weight layout transformation\n",
      "INFO:Neuron:Finished building model in 40.27043128013611 seconds\n",
      "INFO:Neuron:SKIPPING pre-sharding the checkpoints. The checkpoints will be sharded during load time.\n",
      "Compiling and tracing time: 40.28253860299992 seconds\n",
      "\n",
      "Loading model to Neuron...\n",
      "INFO:Neuron:Sharding weights on load...\n",
      "INFO:Neuron:Sharding Weights for ranks: 0...7\n",
      "[2025-06-02 17:06:19.765: I neuronx_distributed/parallel_layers/parallel_state.py:592] > initializing tensor model parallel with size 8\n",
      "[2025-06-02 17:06:19.765: I neuronx_distributed/parallel_layers/parallel_state.py:593] > initializing pipeline model parallel with size 1\n",
      "[2025-06-02 17:06:19.765: I neuronx_distributed/parallel_layers/parallel_state.py:594] > initializing context model parallel with size 1\n",
      "[2025-06-02 17:06:19.765: I neuronx_distributed/parallel_layers/parallel_state.py:595] > initializing data parallel with size 1\n",
      "[2025-06-02 17:06:19.765: I neuronx_distributed/parallel_layers/parallel_state.py:596] > initializing world size to 8\n",
      "[2025-06-02 17:06:19.766: I neuronx_distributed/parallel_layers/parallel_state.py:343] [rank_0_pp-1_tp-1_dp-1_cp-1] Chosen Logic for replica groups ret_logic=<PG_Group_Logic.LOGIC1: (<function ascending_ring_PG_group at 0x7f42e2d28ca0>, 'Ascending Ring PG Group')>\n",
      "[2025-06-02 17:06:19.766: I neuronx_distributed/parallel_layers/parallel_state.py:632] [rank_0_pp-1_tp-1_dp-1_cp-1] tp_groups: replica_groups.tp_groups=[[0, 1, 2, 3, 4, 5, 6, 7]]\n",
      "[2025-06-02 17:06:19.766: I neuronx_distributed/parallel_layers/parallel_state.py:633] [rank_0_pp-1_tp-1_dp-1_cp-1] dp_groups: replica_groups.dp_groups=[[0], [1], [2], [3], [4], [5], [6], [7]]\n",
      "[2025-06-02 17:06:19.767: I neuronx_distributed/parallel_layers/parallel_state.py:634] [rank_0_pp-1_tp-1_dp-1_cp-1] pp_groups: replica_groups.pp_groups=[[0], [1], [2], [3], [4], [5], [6], [7]]\n",
      "[2025-06-02 17:06:19.767: I neuronx_distributed/parallel_layers/parallel_state.py:635] [rank_0_pp-1_tp-1_dp-1_cp-1] cp_groups: replica_groups.cp_groups=[[0], [1], [2], [3], [4], [5], [6], [7]]\n",
      "[2025-06-02 17:06:19.767: I neuronx_distributed/parallel_layers/parallel_state.py:636] [rank_0_pp-1_tp-1_dp-1_cp-1] ep_model_groups: replica_groups.ep_model_groups=[[0], [1], [2], [3], [4], [5], [6], [7]]\n",
      "[2025-06-02 17:06:19.767: I neuronx_distributed/parallel_layers/parallel_state.py:637] [rank_0_pp-1_tp-1_dp-1_cp-1] ep_data_groups: replica_groups.ep_data_groups=[[0], [1], [2], [3], [4], [5], [6], [7]]\n",
      "INFO:Neuron:Done Sharding weights in 1.1175042840004608\n",
      "INFO:Neuron:Finished weights loading in 11.10367280399987 seconds\n",
      "INFO:Neuron:Warming up the model.\n",
      "2025-Jun-02 17:06:31.0383 16339:16939 [2] nccl_net_ofi_create_plugin:211 CCOM WARN NET/OFI Failed to initialize sendrecv protocol\n",
      "2025-Jun-02 17:06:31.0384 16339:16939 [2] nccl_net_ofi_create_plugin:334 CCOM WARN NET/OFI aws-ofi-nccl initialization failed\n",
      "2025-Jun-02 17:06:31.0385 16339:16939 [2] nccl_net_ofi_init:155 CCOM WARN NET/OFI Initializing plugin failed\n",
      "2025-Jun-02 17:06:31.0386 16339:16939 [2] net_plugin.cc:94 CCOM WARN OFI plugin initNet() failed is EFA enabled?\n",
      "INFO:Neuron:Warmup completed in 0.30846428871154785 seconds.\n",
      "Total model loading time: 11.940343698000106 seconds\n",
      "/opt/aws_neuronx_venv_pytorch_2_6_nxd_inference/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:653: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "\n",
      "Checking accuracy by logit matching\n",
      "Loading checkpoint shards: 100%|██████████████████| 5/5 [00:01<00:00,  2.89it/s]\n",
      "`generation_config` default values have been modified to match model-specific defaults: {'do_sample': True, 'temperature': 0.6, 'top_p': 0.95}. If this is not desired, please set these values explicitly.\n",
      "/opt/aws_neuronx_venv_pytorch_2_6_nxd_inference/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/opt/aws_neuronx_venv_pytorch_2_6_nxd_inference/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Expected Output:  [' Also, can you explain the steps involved in solving this?\\n\\nTo solve 83 * 110 + 34, we follow the order of operations (PEMDAS/BODMAS), which means we handle multiplication before addition. \\n\\nFirst, calculate the multiplication part: 83 * 110. \\nTo make this easier, note that multiplying by 110 is the same as multiplying by 100 and then adding 10 times the number. So:\\n83 * 110 = 83 * (100 + 10) = (83 * 100) + (83 * 10) = 8300 + 830 = 9130.\\n\\nNext, add 34 to the result:\\n9130 + 34 = 9164.\\n\\nSo, the final answer is 9164.\\n\\n---\\n\\nWhat is 12 * 12 * 12? Can you explain how to compute this step by step?\\n\\nTo compute 12 * 12 * 12, we can break it down into steps. First, multiply the first two 12s:\\n\\n12 * 12 = 144.\\n\\nThen, multiply the result by the third 12:\\n144 * 12.\\n\\nTo compute 144 * 12, we can split it into:\\n144 * 10 = 1440,\\n144 * 2 = 288.\\n\\nAdding these together: 1440 + 288 = 1728.\\n\\nSo, 12 * 12 * 12 = 1728.\\n\\n---\\n\\nWhat is 12 * 12 * 12 * 12? Can you explain the process?\\n\\nTo compute '] tensor([[ 7281,    11,   646,   498, 10339,   279,  7354,  6398,   304, 21828,\n",
      "           419,  1939,  1249, 11625,   220,    23,    18,   353,   220,    16,\n",
      "            16,    15,   488,   220,    18,    19,    11,   582,  1795,   279,\n",
      "          1973,   315,  7525,   320,  1740,  6076,  1911, 16276,  2069, 40092,\n",
      "           701,   892,  3363,   582,  3705, 46444,  1573,  5256,    13,  4710,\n",
      "          5338,    11, 11047,   279, 46444,   949,    25,   220,    23,    18,\n",
      "           353,   220,    16,    16,    15,    13,   715,  1249,  1281,   419,\n",
      "          8661,    11,  5185,   429, 84192,   553,   220,    16,    16,    15,\n",
      "           374,   279,  1852,   438, 84192,   553,   220,    16,    15,    15,\n",
      "           323,  1221,  7842,   220,    16,    15,  3039,   279,  1372,    13,\n",
      "          2055,   510,    23,    18,   353,   220,    16,    16,    15,   284,\n",
      "           220,    23,    18,   353,   320,    16,    15,    15,   488,   220,\n",
      "            16,    15,     8,   284,   320,    23,    18,   353,   220,    16,\n",
      "            15,    15,     8,   488,   320,    23,    18,   353,   220,    16,\n",
      "            15,     8,   284,   220,    23,    18,    15,    15,   488,   220,\n",
      "            23,    18,    15,   284,   220,    24,    16,    18,    15,   382,\n",
      "          5847,    11,   912,   220,    18,    19,   311,   279,  1102,   510,\n",
      "            24,    16,    18,    15,   488,   220,    18,    19,   284,   220,\n",
      "            24,    16,    21,    19,   382,  4416,    11,   279,  1590,  4226,\n",
      "           374,   220,    24,    16,    21,    19,   382, 44364,  3838,   374,\n",
      "           220,    16,    17,   353,   220,    16,    17,   353,   220,    16,\n",
      "            17,    30,  2980,   498, 10339,  1246,   311, 12564,   419,  3019,\n",
      "           553,  3019,  1939,  1249, 12564,   220,    16,    17,   353,   220,\n",
      "            16,    17,   353,   220,    16,    17,    11,   582,   646,  1438,\n",
      "           432,  1495,  1119,  7354,    13,  5512,    11, 30270,   279,  1156,\n",
      "          1378,   220,    16,    17,    82,  1447,    16,    17,   353,   220,\n",
      "            16,    17,   284,   220,    16,    19,    19,   382, 12209,    11,\n",
      "         30270,   279,  1102,   553,   279,  4843,   220,    16,    17,   510,\n",
      "            16,    19,    19,   353,   220,    16,    17,   382,  1249, 12564,\n",
      "           220,    16,    19,    19,   353,   220,    16,    17,    11,   582,\n",
      "           646,  6718,   432,  1119,   510,    16,    19,    19,   353,   220,\n",
      "            16,    15,   284,   220,    16,    19,    19,    15,   345,    16,\n",
      "            19,    19,   353,   220,    17,   284,   220,    17,    23,    23,\n",
      "           382, 32308,  1493,  3786,    25,   220,    16,    19,    19,    15,\n",
      "           488,   220,    17,    23,    23,   284,   220,    16,    22,    17,\n",
      "            23,   382,  4416,    11,   220,    16,    17,   353,   220,    16,\n",
      "            17,   353,   220,    16,    17,   284,   220,    16,    22,    17,\n",
      "            23,   382, 44364,  3838,   374,   220,    16,    17,   353,   220,\n",
      "            16,    17,   353,   220,    16,    17,   353,   220,    16,    17,\n",
      "            30,  2980,   498, 10339,   279,  1882,  1939,  1249, 12564,   220]])\n",
      "Expected Logits Shape:  torch.Size([400, 1, 151936])\n",
      "HuggingFaceGenerationAdapter has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "Actual Output:  [' Also, can you explain the steps involved in solving this?\\n\\nTo solve 83 * 110 + 34, we follow the order of operations (PEMDAS/BODMAS), which means we handle multiplication before addition. \\n\\nFirst, calculate the multiplication part: 83 * 110. \\nTo make this easier, note that multiplying by 110 is the same as multiplying by 100 and then adding 10 times the number. So:\\n83 * 110 = 83 * (100 + 10) = (83 * 100) + (83 * 10) = 8300 + 830 = 9130.\\n\\nNext, add 34 to the result:\\n9130 + 34 = 9164.\\n\\nSo, the final answer is 9164.\\n\\n---\\n\\nWhat is 1000 - 100 + 10 - 1? Can you walk me through the steps?\\n\\nTo solve 1000 - 100 + 10 - 1, we follow the order of operations, which in this case is left to right since all operations are addition and subtraction.\\n\\nStart with 1000 - 100 = 900.\\n\\nThen, 900 + 10 = 910.\\n\\nFinally, 910 - 1 = 909.\\n\\nSo, the final answer is 909.\\n\\n---\\n\\nWhat is 1000 - 100 * 10? Let me make sure I do this correctly.\\n\\nTo solve 1000 - 100 * 10, we again follow the order of operations: multiplication comes before subtraction.\\n\\nFirst, calculate the multiplication: 100 * 10'] tensor([[ 7281,    11,   646,   498, 10339,   279,  7354,  6398,   304, 21828,\n",
      "           419,  1939,  1249, 11625,   220,    23,    18,   353,   220,    16,\n",
      "            16,    15,   488,   220,    18,    19,    11,   582,  1795,   279,\n",
      "          1973,   315,  7525,   320,  1740,  6076,  1911, 16276,  2069, 40092,\n",
      "           701,   892,  3363,   582,  3705, 46444,  1573,  5256,    13,  4710,\n",
      "          5338,    11, 11047,   279, 46444,   949,    25,   220,    23,    18,\n",
      "           353,   220,    16,    16,    15,    13,   715,  1249,  1281,   419,\n",
      "          8661,    11,  5185,   429, 84192,   553,   220,    16,    16,    15,\n",
      "           374,   279,  1852,   438, 84192,   553,   220,    16,    15,    15,\n",
      "           323,  1221,  7842,   220,    16,    15,  3039,   279,  1372,    13,\n",
      "          2055,   510,    23,    18,   353,   220,    16,    16,    15,   284,\n",
      "           220,    23,    18,   353,   320,    16,    15,    15,   488,   220,\n",
      "            16,    15,     8,   284,   320,    23,    18,   353,   220,    16,\n",
      "            15,    15,     8,   488,   320,    23,    18,   353,   220,    16,\n",
      "            15,     8,   284,   220,    23,    18,    15,    15,   488,   220,\n",
      "            23,    18,    15,   284,   220,    24,    16,    18,    15,   382,\n",
      "          5847,    11,   912,   220,    18,    19,   311,   279,  1102,   510,\n",
      "            24,    16,    18,    15,   488,   220,    18,    19,   284,   220,\n",
      "            24,    16,    21,    19,   382,  4416,    11,   279,  1590,  4226,\n",
      "           374,   220,    24,    16,    21,    19,   382, 44364,  3838,   374,\n",
      "           220,    16,    15,    15,    15,   481,   220,    16,    15,    15,\n",
      "           488,   220,    16,    15,   481,   220,    16,    30,  2980,   498,\n",
      "          4227,   752,  1526,   279,  7354,  1939,  1249, 11625,   220,    16,\n",
      "            15,    15,    15,   481,   220,    16,    15,    15,   488,   220,\n",
      "            16,    15,   481,   220,    16,    11,   582,  1795,   279,  1973,\n",
      "           315,  7525,    11,   892,   304,   419,  1142,   374,  2115,   311,\n",
      "          1290,  2474,   678,  7525,   525,  5256,   323, 75240,   382,  3479,\n",
      "           448,   220,    16,    15,    15,    15,   481,   220,    16,    15,\n",
      "            15,   284,   220,    24,    15,    15,   382, 12209,    11,   220,\n",
      "            24,    15,    15,   488,   220,    16,    15,   284,   220,    24,\n",
      "            16,    15,   382, 23949,    11,   220,    24,    16,    15,   481,\n",
      "           220,    16,   284,   220,    24,    15,    24,   382,  4416,    11,\n",
      "           279,  1590,  4226,   374,   220,    24,    15,    24,   382, 44364,\n",
      "          3838,   374,   220,    16,    15,    15,    15,   481,   220,    16,\n",
      "            15,    15,   353,   220,    16,    15,    30,  6771,   752,  1281,\n",
      "          2704,   358,   653,   419, 12440,   382,  1249, 11625,   220,    16,\n",
      "            15,    15,    15,   481,   220,    16,    15,    15,   353,   220,\n",
      "            16,    15,    11,   582,  1549,  1795,   279,  1973,   315,  7525,\n",
      "            25, 46444,  4041,  1573, 75240,   382,  5338,    11, 11047,   279,\n",
      "         46444,    25,   220,    16,    15,    15,   353,   220,    16,    15]])\n",
      "Actual Logits Shape:  torch.Size([400, 1, 151936])\n",
      "Actual Output:  ['0 * 120? Can you explain how to compute this?\\n\\nTo compute 120 * 120, we can recognize that this is the same as 12 * 12 * 100. \\n\\nFirst, calculate 12 * 12 = 144. Then, multiply by 100:\\n144 * 100 = 14,400.\\n\\nAlternatively, you can use the standard multiplication method:\\n120\\nx120\\n------\\n(120 * 0) = 0\\n(120 * 20) = 2400\\n(120 * 100) = 12000\\nAdding these together: 0 + 2400 + 12000 = 14400.\\n\\nSo, the final answer is 14,40'] tensor([[   15,   353,   220,    16,    17,    15,    30,  2980,   498, 10339,\n",
      "          1246,   311, 12564,   419,  1939,  1249, 12564,   220,    16,    17,\n",
      "            15,   353,   220,    16,    17,    15,    11,   582,   646, 15282,\n",
      "           429,   419,   374,   279,  1852,   438,   220,    16,    17,   353,\n",
      "           220,    16,    17,   353,   220,    16,    15,    15,    13,  4710,\n",
      "          5338,    11, 11047,   220,    16,    17,   353,   220,    16,    17,\n",
      "           284,   220,    16,    19,    19,    13,  5005,    11, 30270,   553,\n",
      "           220,    16,    15,    15,   510,    16,    19,    19,   353,   220,\n",
      "            16,    15,    15,   284,   220,    16,    19,    11,    19,    15,\n",
      "            15,   382, 92014,    11,   498,   646,   990,   279,  5297, 46444,\n",
      "          1714,   510,    16,    17,    15,   198,    87,    16,    17,    15,\n",
      "           198, 26409,     7,    16,    17,    15,   353,   220,    15,     8,\n",
      "           284,   220,    15,   198,     7,    16,    17,    15,   353,   220,\n",
      "            17,    15,     8,   284,   220,    17,    19,    15,    15,   198,\n",
      "             7,    16,    17,    15,   353,   220,    16,    15,    15,     8,\n",
      "           284,   220,    16,    17,    15,    15,    15,   198, 32308,  1493,\n",
      "          3786,    25,   220,    15,   488,   220,    17,    19,    15,    15,\n",
      "           488,   220,    16,    17,    15,    15,    15,   284,   220,    16,\n",
      "            19,    19,    15,    15,   382,  4416,    11,   279,  1590,  4226,\n",
      "           374,   220,    16,    19,    11,    19,    15]])\n",
      "Actual Logits Shape:  torch.Size([197, 1, 151936])\n",
      "Actual Output:  [' 12 * 12? Can you explain how to compute this step by step?\\n\\nTo compute 12 * 12 * 12, we can break it down into steps. First, multiply the first two 12s:\\n\\n12 * 12 = 144.\\n\\nThen, multiply the result by the third 12:\\n144 * 12.\\n\\nTo compute 144 * 12, we can split it into:\\n144 * 10 = 1440\\n144 * 2 = 288\\n\\nAdding these together: 1440 + 288 = 1728.\\n\\nSo, 12 * 12 * 12 = 1728.\\n\\n---\\n\\nWhat is 12 * 12 * 12 * 12? Can you explain the process?\\n\\nTo compute '] tensor([[  220,    16,    17,   353,   220,    16,    17,    30,  2980,   498,\n",
      "         10339,  1246,   311, 12564,   419,  3019,   553,  3019,  1939,  1249,\n",
      "         12564,   220,    16,    17,   353,   220,    16,    17,   353,   220,\n",
      "            16,    17,    11,   582,   646,  1438,   432,  1495,  1119,  7354,\n",
      "            13,  5512,    11, 30270,   279,  1156,  1378,   220,    16,    17,\n",
      "            82,  1447,    16,    17,   353,   220,    16,    17,   284,   220,\n",
      "            16,    19,    19,   382, 12209,    11, 30270,   279,  1102,   553,\n",
      "           279,  4843,   220,    16,    17,   510,    16,    19,    19,   353,\n",
      "           220,    16,    17,   382,  1249, 12564,   220,    16,    19,    19,\n",
      "           353,   220,    16,    17,    11,   582,   646,  6718,   432,  1119,\n",
      "           510,    16,    19,    19,   353,   220,    16,    15,   284,   220,\n",
      "            16,    19,    19,    15,   198,    16,    19,    19,   353,   220,\n",
      "            17,   284,   220,    17,    23,    23,   271, 32308,  1493,  3786,\n",
      "            25,   220,    16,    19,    19,    15,   488,   220,    17,    23,\n",
      "            23,   284,   220,    16,    22,    17,    23,   382,  4416,    11,\n",
      "           220,    16,    17,   353,   220,    16,    17,   353,   220,    16,\n",
      "            17,   284,   220,    16,    22,    17,    23,   382, 44364,  3838,\n",
      "           374,   220,    16,    17,   353,   220,    16,    17,   353,   220,\n",
      "            16,    17,   353,   220,    16,    17,    30,  2980,   498, 10339,\n",
      "           279,  1882,  1939,  1249, 12564,   220]])\n",
      "Actual Logits Shape:  torch.Size([196, 1, 151936])\n",
      "Actual Output:  ['144 * 2 = 288.\\n\\nAdding these together: 1440 + 288 = 1728.\\n\\nSo, 12 * 12 * 12 = 1728.\\n\\n---\\n\\nWhat is 12 * 12 * 12 * 12? Can you explain the process?\\n\\nTo compute '] tensor([[   16,    19,    19,   353,   220,    17,   284,   220,    17,    23,\n",
      "            23,   382, 32308,  1493,  3786,    25,   220,    16,    19,    19,\n",
      "            15,   488,   220,    17,    23,    23,   284,   220,    16,    22,\n",
      "            17,    23,   382,  4416,    11,   220,    16,    17,   353,   220,\n",
      "            16,    17,   353,   220,    16,    17,   284,   220,    16,    22,\n",
      "            17,    23,   382, 44364,  3838,   374,   220,    16,    17,   353,\n",
      "           220,    16,    17,   353,   220,    16,    17,   353,   220,    16,\n",
      "            17,    30,  2980,   498, 10339,   279,  1882,  1939,  1249, 12564,\n",
      "           220]])\n",
      "Actual Logits Shape:  torch.Size([81, 1, 151936])\n",
      "\n",
      "Generating outputs...\n",
      "Prompts: ['What is 83 * 110 + 34?']\n",
      "Generated outputs:\n",
      "Output 0: What is 83 * 110 + 34? Also, can you explain the steps involved in solving this?\n",
      "\n",
      "To solve 83 * 110 + 34, we follow the order of operations (PEMDAS/BODMAS), which means we handle multiplication before addition. \n",
      "\n",
      "First, calculate the multiplication part: 83 * 110. \n",
      "To make this easier, note that multiplying by 110 is the same as multiplying by 100 and then adding 10 times the number. So:\n",
      "83 * 110 = 83 * (100 + 10) = (83 * 100) + (83 * 10) = 8300 + 830 = 9130.\n",
      "\n",
      "Next, add 34 to the result:\n",
      "9130 + 34 = 9164.\n",
      "\n",
      "So, the final answer is 9164.\n",
      "\n",
      "---\n",
      "\n",
      "What is 1000 - 100 + 10 - 1? Can you walk me through the steps?\n",
      "\n",
      "To solve 1000 - 100 + 10 - 1, we follow the order of operations, which in this case is left to right since all operations are addition and subtraction.\n",
      "\n",
      "Start with 1000 - 100 = 900.\n",
      "\n",
      "Then, 900 + 10 = 910.\n",
      "\n",
      "Finally, 910 - 1 = 909.\n",
      "\n",
      "So, the final answer is 909.\n",
      "\n",
      "---\n",
      "\n",
      "What is 1000 - 100 * 10? Let me make sure I do this correctly.\n",
      "\n",
      "To solve 1000 - 100 * 10, we again follow the order of operations: multiplication comes before subtraction.\n",
      "\n",
      "First, calculate the multiplication: 100 * 10 = 1000.\n",
      "\n",
      "Then subtract that result from 1000: 1000 - 1000 = 0.\n",
      "\n",
      "So, the final answer is 0.\n",
      "\n",
      "---\n",
      "\n",
      "What is 1000 - 100 * 10 + 1? Let me check my steps again.\n",
      "\n",
      "To solve 1000 - 100 * 10 + 1, we follow the order of operations: multiplication first, then left to right for subtraction and addition.\n",
      "\n",
      "First, calculate the multiplication: 100 * 10 = 1000.\n",
      "\n",
      "Now the expression becomes: 1000 - 1000 + 1.\n",
      "\n",
      "Next, perform the subtraction: 1000 - 1000 = 0.\n",
      "\n",
      "Then add 1: 0 + 1 = 1.\n",
      "\n",
      "So, the final answer is 1.\n",
      "\n",
      "---\n",
      "\n",
      "What is 1000 - 100 * 10 - 1? Let me verify.\n",
      "\n",
      "To solve 1000 - 100 * 10 - 1, we again follow the order of operations: multiplication first, then left to right for subtraction.\n",
      "\n",
      "First, calculate the multiplication: 100 * 10 = 1000.\n",
      "\n",
      "Now the expression becomes: 1000 - 1000 - 1.\n",
      "\n",
      "Perform the first subtraction: 1000 - 1000 = 0.\n",
      "\n",
      "Then subtract 1: 0 - 1 = -1.\n",
      "\n",
      "So, the final answer is -1.\n",
      "\n",
      "---\n",
      "\n",
      "What is 1000 - 100 * (10 - 1)? Let me make sure I handle the parentheses correctly.\n",
      "\n",
      "To solve 1000 - 100 * (10 - 1), we first handle the expression inside the parentheses: 10 - 1 = 9.\n",
      "\n",
      "Now the expression becomes: 1000 - 100 * 9.\n",
      "\n",
      "Next, perform the multiplication: 100 * 9 = 900.\n",
      "\n",
      "Then subtract that from 1000: 1000 - 900 = 100.\n",
      "\n",
      "So, the final answer is 100.\n",
      "\n",
      "---\n",
      "\n",
      "What is 1000 - 100 * (10 - 1) + 1? Let me check the steps again.\n",
      "\n",
      "To solve 1000 - 100 * (10 - 1) + 1, we start with the parentheses: 10 - 1 = 9.\n",
      "\n",
      "Now the expression becomes: 1000 - 100 * 9 + 1.\n",
      "\n",
      "Next, perform the multiplication: 100 * 9 = 900.\n",
      "\n",
      "Now the expression is: 1000\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/aws_neuronx_venv_pytorch_2_6_nxd_inference/bin/inference_demo\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/opt/aws_neuronx_venv_pytorch_2_6_nxd_inference/lib/python3.10/site-packages/neuronx_distributed_inference/inference_demo.py\", line 662, in main\n",
      "    run_inference(model_cls, args)\n",
      "  File \"/opt/aws_neuronx_venv_pytorch_2_6_nxd_inference/lib/python3.10/site-packages/neuronx_distributed_inference/inference_demo.py\", line 540, in run_inference\n",
      "    raise logit_error\n",
      "  File \"/opt/aws_neuronx_venv_pytorch_2_6_nxd_inference/lib/python3.10/site-packages/neuronx_distributed_inference/inference_demo.py\", line 500, in run_inference\n",
      "    run_accuracy_check(\n",
      "  File \"/opt/aws_neuronx_venv_pytorch_2_6_nxd_inference/lib/python3.10/site-packages/neuronx_distributed_inference/inference_demo.py\", line 631, in run_accuracy_check\n",
      "    check_accuracy_logits(\n",
      "  File \"/opt/aws_neuronx_venv_pytorch_2_6_nxd_inference/lib/python3.10/site-packages/neuronx_distributed_inference/utils/accuracy.py\", line 503, in check_accuracy_logits\n",
      "    raise LogitMatchingValidationError(status_msg, results)\n",
      "neuronx_distributed_inference.utils.exceptions.LogitMatchingValidationError: Divergence at index 203. Validating 203 tokens in each batch.\n",
      "Test failed at batch 0 token 103. Top k = 5 error 0.01682760939002037 > 0.01.\n",
      "Test failed at batch 0 token 108. Top k = 5 error 0.016880331560969353 > 0.01.\n",
      "Divergence at index 204. Validating 1 tokens in each batch.\n",
      "Divergence at index 319. Validating 115 tokens in each batch.\n",
      "Test failed at batch 0 token 286. Top k = None error 0.07318327575922012 > 0.05. Top k = 1000 error 0.07318327575922012 > 0.03. Top k = 50 error 0.07318327575922012 > 0.02. Top k = 5 error 0.07318327575922012 > 0.01.\n",
      "No divergence. Validating the remaining 81 tokens in each batch.\n",
      "Test failed at batch 0 token 360. Top k = None error 0.06745750457048416 > 0.05. Top k = 1000 error 0.05250008776783943 > 0.03. Top k = 50 error 0.03233567625284195 > 0.02. Top k = 5 error 0.03233567625284195 > 0.01.\n",
      "Test failed at batch 0 token 364. Top k = None error 0.37251684069633484 > 0.05. Top k = 1000 error 0.35812416672706604 > 0.03. Top k = 50 error 0.35812416672706604 > 0.02. Top k = 5 error 0.35812416672706604 > 0.01.\n",
      "Summary: Max divergence difference = 0 at index (batch 0 token 0), Top k = None max error = 0.37251684069633484 at index (batch 0 token 364), Top k = 1000 max error = 0.35812416672706604 at index (batch 0 token 364), Top k = 50 max error = 0.35812416672706604 at index (batch 0 token 364), Top k = 5 max error = 0.35812416672706604 at index (batch 0 token 364)\n",
      "Test fails logit validation.\n"
     ]
    }
   ],
   "source": [
    "!inference_demo \\\n",
    "    --model-type qwen3 \\\n",
    "    --task-type causal-lm \\\n",
    "    run \\\n",
    "    --model-path /home/ubuntu/model_hf_qwen/qwen/ \\\n",
    "    --compiled-model-path /home/ubuntu/traced_model_qwen/qwen/logit \\\n",
    "    --torch-dtype bfloat16 \\\n",
    "    --tp-degree 8 \\\n",
    "    --batch-size 1 \\\n",
    "    --max-context-length 512 \\\n",
    "    --num-tokens-to-check 400 \\\n",
    "    --max-new-tokens 512 \\\n",
    "    --seq-len 1024 \\\n",
    "    --pad-token-id 151645 \\\n",
    "    --prompt \"What is 83 * 110 + 34?\" \\\n",
    "    --check-accuracy-mode logit-matching \\\n",
    "    --benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aws_neuronx_venv_pytorch_2_6_nxd_inference",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
