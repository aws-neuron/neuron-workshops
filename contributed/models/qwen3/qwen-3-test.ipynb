{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall transformers --y\n",
    "!pip install transformers==4.51.3\n",
    "\n",
    "# Installing collected packages: transformers\n",
    "# ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
    "# neuronx-distributed-inference 0.3.5591+f50feae2 requires transformers==4.48.*, but you have transformers 4.51.3 which is incompatible.\n",
    "# Successfully installed transformers-4.51.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You may ignore the error that nxdi is not compatible with transformers ==4.48.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "libneuronxla                  2.2.3493.0+78c3e78c\n",
      "neuronx-cc                    2.18.121.0+9e31e41a\n",
      "neuronx-distributed           0.12.12111+cdd84048\n",
      "neuronx-distributed-inference 0.3.5591+f50feae2\n",
      "torch-neuronx                 2.6.0.2.7.5413+113e6810\n",
      "transformers                  4.51.3\n"
     ]
    }
   ],
   "source": [
    "!pip list | grep neuron\n",
    "!pip list | grep transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, GenerationConfig\n",
    "from neuronx_distributed_inference.models.config import NeuronConfig, OnDeviceSamplingConfig\n",
    "from neuronx_distributed_inference.utils.hf_adapter import HuggingFaceGenerationAdapter, load_pretrained_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/home/ubuntu/model_hf_qwen/qwen/\"\n",
    "traced_model_path = \"/home/ubuntu/traced_model_qwen3/qwen3/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "snapshot_download(\"Qwen/Qwen3-8B\", local_dir=model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modeling_qwen3 import Qwen3InferenceConfig, NeuronQwen3ForCausalLM\n",
    "\n",
    "def run_qwen3_compile():\n",
    "    # Initialize configs and tokenizer.\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path, padding_side=\"right\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    generation_config = GenerationConfig.from_pretrained(model_path)\n",
    "    generation_config_kwargs = {\n",
    "        \"do_sample\": True,\n",
    "        \"top_k\": 1,\n",
    "        \"pad_token_id\": tokenizer.pad_token_id,\n",
    "    }\n",
    "    generation_config.update(**generation_config_kwargs)\n",
    " \n",
    "    neuron_config = NeuronConfig(\n",
    "        tp_degree=8,\n",
    "        batch_size=1,\n",
    "        max_context_length=1024, \n",
    "        seq_len=2048, \n",
    "        on_device_sampling_config=OnDeviceSamplingConfig(top_k=5),\n",
    "        enable_bucketing=True,\n",
    "        context_encoding_buckets=[1024],\n",
    "        token_generation_buckets=[2048],\n",
    "        flash_decoding_enabled=False,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        fused_qkv=False,\n",
    "        attn_kernel_enabled=True,\n",
    "        attn_cls=\"NeuronQwen3Attention\"\n",
    "    )\n",
    "    config = Qwen3InferenceConfig(\n",
    "        neuron_config,\n",
    "        load_config=load_pretrained_config(model_path),\n",
    "    )\n",
    "    \n",
    "    # Compile and save model.\n",
    "    print(\"\\nCompiling and saving model...\")\n",
    "    model = NeuronQwen3ForCausalLM(model_path, config)\n",
    "    model.compile(traced_model_path)\n",
    "    tokenizer.save_pretrained(traced_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_qwen3_compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modeling_qwen3 import Qwen3InferenceConfig, NeuronQwen3ForCausalLM\n",
    "\n",
    "model = NeuronQwen3ForCausalLM(traced_model_path)\n",
    "model.load(traced_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neuronx_distributed_inference.models.config.NeuronConfig"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = model.get_config_cls()\n",
    "config.get_neuron_config_cls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.num_attention_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.num_key_value_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4096"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(traced_model_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "generation_config = GenerationConfig.from_pretrained(model_path)\n",
    "generation_config_kwargs = {\n",
    "    \"do_sample\": False,\n",
    "    \"temperature\": 0.9,\n",
    "    \"top_k\": 5,\n",
    "    \"pad_token_id\": tokenizer.pad_token_id,\n",
    "}\n",
    "generation_config.update(**generation_config_kwargs)\n",
    "generation_model = HuggingFaceGenerationAdapter(model)\n",
    "messages = [{'role': 'user', 'content': \"What's your name?\"}]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=False # Switches between thinking and non-thinking modes. Default is True.\n",
    ")\n",
    "inputs = tokenizer([text], return_tensors=\"pt\")\n",
    "input_ids = inputs['input_ids']  \n",
    "\n",
    "outputs = generation_model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_new_tokens=512\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thinking content: \n",
      "content: My name is Qwen, and I'm a large language model developed by Alibaba Cloud. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "output_ids = outputs[0][len(inputs.input_ids[0]):].tolist() \n",
    "\n",
    "# parsing thinking content\n",
    "try:\n",
    "    # rindex finding 151668 (</think>)\n",
    "    index = len(output_ids) - output_ids[::-1].index(151668)\n",
    "except ValueError:\n",
    "    index = 0\n",
    "\n",
    "thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
    "content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
    "\n",
    "print(\"thinking content:\", thinking_content)\n",
    "print(\"content:\", content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thinking example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(traced_model_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "generation_config = GenerationConfig.from_pretrained(model_path)\n",
    "generation_config_kwargs = {\n",
    "    \"do_sample\": False,\n",
    "    \"temperature\": 0.9,\n",
    "    \"top_k\": 5,\n",
    "    \"pad_token_id\": tokenizer.pad_token_id,\n",
    "}\n",
    "generation_config.update(**generation_config_kwargs)\n",
    "generation_model = HuggingFaceGenerationAdapter(model)\n",
    "messages = [{'role': 'system', 'content': \"Only think through one example before providing the correct answer\"},\n",
    "             {'role': 'user', 'content': \"What is 83 * 110 + 34?\"}\n",
    "        ]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=True # Switches between thinking and non-thinking modes. Default is True.\n",
    ")\n",
    "inputs = tokenizer([text], return_tensors=\"pt\")\n",
    "input_ids = inputs['input_ids'] \n",
    "outputs = generation_model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_new_tokens=1024\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thinking content: <think>\n",
      "Okay, let's see. I need to calculate 83 multiplied by 110 and then add 34 to the result. Hmm, let me break this down step by step. First, I should handle the multiplication part: 83 times 110. \n",
      "\n",
      "Wait, multiplying by 110 might be easier if I think of it as multiplying by 100 and then adding 10 times the number. Because 110 is 100 + 10. So, 83 times 100 is 8300, and 83 times 10 is 830. Then adding those two together: 8300 + 830. Let me check that. 8300 plus 800 is 9100, and then plus 30 more would be 9130. So, 83 * 110 equals 9130?\n",
      "\n",
      "Wait, let me verify that another way. Maybe using the standard multiplication method. Let's write it out:\n",
      "\n",
      "   83\n",
      "x110\n",
      "------\n",
      "First, multiply 83 by 0 (the units place of 110), which gives 0.\n",
      "Then multiply 83 by 1 (the tens place of 110), which is 83, but since it's in the tens place, it's actually 830.\n",
      "Then multiply 83 by 1 (the hundreds place of 110), which is 83, but since it's in the hundreds place, it's 8300.\n",
      "Adding those together: 0 + 830 + 8300 = 9130. Okay, that matches my previous result. So 83*110 is indeed 9130.\n",
      "\n",
      "Now, the next part is adding 34 to that result. So 9130 + 34. Let me do that. 9130 plus 30 is 9160, and then plus 4 more is 9164. \n",
      "\n",
      "Wait, let me check again. 9130 + 34. Breaking it down: 9130 + 30 = 9160, then 9160 + 4 = 9164. Yes, that seems right. \n",
      "\n",
      "Alternatively, I can add 34 to 9130 directly. 9130 + 34. The units digit: 0 + 4 = 4. The tens digit: 3 + 3 = 6. The hundreds and above remain the same. So 9164. Yep, that's correct.\n",
      "\n",
      "So putting it all together, 83 multiplied by 110 is 9130, and adding 34 gives 9164. I think that's the right answer. Let me just confirm once more with another method. Maybe using distributive property for the entire expression.\n",
      "\n",
      "Original problem: 83*110 + 34. Let's think of 110 as 100 + 10, so 83*(100 + 10) + 34 = 83*100 + 83*10 + 34 = 8300 + 830 + 34. Adding those: 8300 + 830 is 9130, then 9130 + 34 is 9164. Yep, same result. \n",
      "\n",
      "I think that's solid. No mistakes in the steps. So the final answer should be 9164.\n",
      "</think>\n",
      "################################################################################################################################################################################################################################################################################################################################\n",
      "content: To solve $ 83 \\times 110 + 34 $, we break it into two parts:\n",
      "\n",
      "1. **Multiplication**:  \n",
      "   $ 83 \\times 110 $ can be simplified by recognizing that $ 110 = 100 + 10 $.  \n",
      "   $$\n",
      "   83 \\times 110 = 83 \\times (100 + 10) = (83 \\times 100) + (83 \\times 10) = 8300 + 830 = 9130\n",
      "   $$\n",
      "\n",
      "2. **Addition**:  \n",
      "   Add 34 to the result:  \n",
      "   $$\n",
      "   9130 + 34 = 9164\n",
      "   $$\n",
      "\n",
      "**Final Answer:**  \n",
      "$$\n",
      "\\boxed{9164}\n",
      "$$\n"
     ]
    }
   ],
   "source": [
    "output_ids = outputs[0][len(inputs.input_ids[0]):].tolist() \n",
    "\n",
    "# parsing thinking content\n",
    "try:\n",
    "    # rindex finding 151668 (</think>)\n",
    "    index = len(output_ids) - output_ids[::-1].index(151668)\n",
    "except ValueError:\n",
    "    index = 0\n",
    "\n",
    "thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
    "content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
    "\n",
    "print(\"thinking content:\", thinking_content)\n",
    "print('####'*80)\n",
    "print(\"content:\", content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9164"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans = 83*110+34\n",
    "ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = '/opt/aws_neuronx_venv_pytorch_2_6_nxd_inference/lib/python3.10/site-packages/neuronx_distributed_inference/'\n",
    "!cp modeling_qwen3.py {dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Edit the inference_demo.py file to include the following:\n",
    "\n",
    "```python\n",
    "from .modeling_qwen import NeuronQwen3ForCausalLM\n",
    "\n",
    "MODEL_TYPES = {\n",
    "    \"llama\": {\"causal-lm\": NeuronLlamaForCausalLM},\n",
    "    \"mixtral\": {\"causal-lm\": NeuronMixtralForCausalLM},\n",
    "    \"dbrx\": {\"causal-lm\": NeuronDbrxForCausalLM},\n",
    "    'qwen3': {\"causal-lm\": NeuronQwen3ForCausalLM}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/aws_neuronx_venv_pytorch_2_6_nxd_inference/lib/python3.10/site-packages/neuronx_distributed/modules/moe/expert_mlps.py:11: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from neuronx_distributed.modules.moe.blockwise import (\n",
      "/opt/aws_neuronx_venv_pytorch_2_6_nxd_inference/lib/python3.10/site-packages/neuronx_distributed/modules/moe/expert_mlps.py:11: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from neuronx_distributed.modules.moe.blockwise import (\n",
      "/opt/aws_neuronx_venv_pytorch_2_6_nxd_inference/lib/python3.10/site-packages/neuronx_distributed/modules/moe/expert_mlps.py:11: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from neuronx_distributed.modules.moe.blockwise import (\n",
      "/opt/aws_neuronx_venv_pytorch_2_6_nxd_inference/lib/python3.10/site-packages/neuronx_distributed_inference/modules/attention/utils.py:14: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from neuronx_distributed_inference.modules.custom_calls import neuron_cumsum\n",
      "/opt/aws_neuronx_venv_pytorch_2_6_nxd_inference/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: Set seed for `privateuseone` device does not take effect, please add API's `_is_in_bad_fork` and `manual_seed_all` to `privateuseone` device module.\n",
      "  return fn(*args, **kwargs)\n",
      "/opt/aws_neuronx_venv_pytorch_2_6_nxd_inference/lib/python3.10/site-packages/neuronx_distributed_inference/modules/lora_serving/lora_model.py:12: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from neuronx_distributed_inference.modules.attention.gqa import GQA, GroupQueryAttention_QKV\n",
      "/opt/aws_neuronx_venv_pytorch_2_6_nxd_inference/lib/python3.10/site-packages/neuronx_distributed_inference/modules/lora_serving/lora_model.py:12: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from neuronx_distributed_inference.modules.attention.gqa import GQA, GroupQueryAttention_QKV\n",
      "/opt/aws_neuronx_venv_pytorch_2_6_nxd_inference/lib/python3.10/site-packages/neuronx_distributed_inference/modules/lora_serving/lora_model.py:12: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from neuronx_distributed_inference.modules.attention.gqa import GQA, GroupQueryAttention_QKV\n",
      "/opt/aws_neuronx_venv_pytorch_2_6_nxd_inference/lib/python3.10/site-packages/neuronx_distributed_inference/models/dbrx/modeling_dbrx.py:38: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from neuronx_distributed_inference.modules.attention.attention_base import NeuronAttentionBase\n",
      "/opt/aws_neuronx_venv_pytorch_2_6_nxd_inference/lib/python3.10/site-packages/neuronx_distributed_inference/models/dbrx/modeling_dbrx.py:38: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from neuronx_distributed_inference.modules.attention.attention_base import NeuronAttentionBase\n",
      "/opt/aws_neuronx_venv_pytorch_2_6_nxd_inference/lib/python3.10/site-packages/neuronx_distributed_inference/inference_demo.py:25: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from neuronx_distributed_inference.models.dbrx.modeling_dbrx import NeuronDbrxForCausalLM\n",
      "/opt/aws_neuronx_venv_pytorch_2_6_nxd_inference/lib/python3.10/site-packages/neuronx_distributed_inference/inference_demo.py:27: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from neuronx_distributed_inference.models.mixtral.modeling_mixtral import NeuronMixtralForCausalLM\n",
      "/opt/aws_neuronx_venv_pytorch_2_6_nxd_inference/lib/python3.10/site-packages/neuronx_distributed_inference/models/mllama/modeling_mllama.py:72: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from .modeling_mllama_vision import NeuronMllamaVisionModel  # noqa: E402\n",
      "/opt/aws_neuronx_venv_pytorch_2_6_nxd_inference/lib/python3.10/site-packages/neuronx_distributed_inference/utils/accuracy.py:29: UserWarning: Intel extension for pytorch not found. For faster CPU references install `intel-extension-for-pytorch`.\n",
      "  warnings.warn(\n",
      "/opt/aws_neuronx_venv_pytorch_2_6_nxd_inference/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: Set seed for `privateuseone` device does not take effect, please add API's `_is_in_bad_fork` and `manual_seed_all` to `privateuseone` device module.\n",
      "  return fn(*args, **kwargs)\n",
      "Loading configs...\n",
      "WARNING:root:NeuronConfig init: Unexpected keyword arguments: {'model_type': 'qwen3', 'task_type': 'causal-lm', 'model_path': '/home/ubuntu/model_hf_qwen/qwen/', 'compiled_model_path': '/home/ubuntu/traced_model_qwen/qwen/logit', 'benchmark': True, 'check_accuracy_mode': <CheckAccuracyMode.LOGIT_MATCHING: 'logit-matching'>, 'divergence_difference_tol': 0.001, 'prompts': ['To be, or not to be'], 'top_k': 1, 'top_p': 1.0, 'temperature': 1.0, 'do_sample': False, 'dynamic': False, 'pad_token_id': 151645, 'on_device_sampling': False, 'enable_torch_dist': False, 'enable_lora': False, 'max_loras': 1, 'max_lora_rank': 16, 'skip_warmup': False, 'skip_compile': False, 'compile_only': False, 'compile_dry_run': False, 'hlo_debug': False}\n",
      "\n",
      "Compiling and saving model...\n",
      "INFO:Neuron:Generating HLOs for the following models: ['context_encoding_model', 'token_generation_model']\n",
      "[2025-06-02 14:42:25.152: I neuronx_distributed/parallel_layers/parallel_state.py:592] > initializing tensor model parallel with size 8\n",
      "[2025-06-02 14:42:25.152: I neuronx_distributed/parallel_layers/parallel_state.py:593] > initializing pipeline model parallel with size 1\n",
      "[2025-06-02 14:42:25.152: I neuronx_distributed/parallel_layers/parallel_state.py:594] > initializing context model parallel with size 1\n",
      "[2025-06-02 14:42:25.152: I neuronx_distributed/parallel_layers/parallel_state.py:595] > initializing data parallel with size 1\n",
      "[2025-06-02 14:42:25.153: I neuronx_distributed/parallel_layers/parallel_state.py:596] > initializing world size to 8\n",
      "[2025-06-02 14:42:25.153: I neuronx_distributed/parallel_layers/parallel_state.py:343] [rank_0_pp-1_tp-1_dp-1_cp-1] Chosen Logic for replica groups ret_logic=<PG_Group_Logic.LOGIC1: (<function ascending_ring_PG_group at 0x7ca44a1b7a30>, 'Ascending Ring PG Group')>\n",
      "[2025-06-02 14:42:25.154: I neuronx_distributed/parallel_layers/parallel_state.py:632] [rank_0_pp-1_tp-1_dp-1_cp-1] tp_groups: replica_groups.tp_groups=[[0, 1, 2, 3, 4, 5, 6, 7]]\n",
      "[2025-06-02 14:42:25.154: I neuronx_distributed/parallel_layers/parallel_state.py:633] [rank_0_pp-1_tp-1_dp-1_cp-1] dp_groups: replica_groups.dp_groups=[[0], [1], [2], [3], [4], [5], [6], [7]]\n",
      "[2025-06-02 14:42:25.154: I neuronx_distributed/parallel_layers/parallel_state.py:634] [rank_0_pp-1_tp-1_dp-1_cp-1] pp_groups: replica_groups.pp_groups=[[0], [1], [2], [3], [4], [5], [6], [7]]\n",
      "[2025-06-02 14:42:25.154: I neuronx_distributed/parallel_layers/parallel_state.py:635] [rank_0_pp-1_tp-1_dp-1_cp-1] cp_groups: replica_groups.cp_groups=[[0], [1], [2], [3], [4], [5], [6], [7]]\n",
      "[2025-06-02 14:42:25.154: I neuronx_distributed/parallel_layers/parallel_state.py:636] [rank_0_pp-1_tp-1_dp-1_cp-1] ep_model_groups: replica_groups.ep_model_groups=[[0], [1], [2], [3], [4], [5], [6], [7]]\n",
      "[2025-06-02 14:42:25.154: I neuronx_distributed/parallel_layers/parallel_state.py:637] [rank_0_pp-1_tp-1_dp-1_cp-1] ep_data_groups: replica_groups.ep_data_groups=[[0], [1], [2], [3], [4], [5], [6], [7]]\n",
      "INFO:Neuron:Generating 1 hlos for key: context_encoding_model\n",
      "INFO:Neuron:Started loading module context_encoding_model\n",
      "INFO:Neuron:Finished loading module context_encoding_model in 0.0800018310546875 seconds\n",
      "INFO:Neuron:generating HLO: context_encoding_model, input example shape = torch.Size([1, 16])\n",
      "/opt/aws_neuronx_venv_pytorch_2_6_nxd_inference/lib/python3.10/site-packages/neuronx_distributed/parallel_layers/layers.py:478: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=False):\n",
      "/opt/aws_neuronx_venv_pytorch_2_6_nxd_inference/lib/python3.10/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:210: UserWarning: Received an input tensor that was unused. Tensor will be ignored. (index=1, shape=torch.Size([1, 16]), dtype=torch.int32)\n",
      "  warnings.warn(\n",
      "/opt/aws_neuronx_venv_pytorch_2_6_nxd_inference/lib/python3.10/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:210: UserWarning: Received an input tensor that was unused. Tensor will be ignored. (index=3, shape=torch.Size([1]), dtype=torch.int32)\n",
      "  warnings.warn(\n",
      "/opt/aws_neuronx_venv_pytorch_2_6_nxd_inference/lib/python3.10/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:210: UserWarning: Received an input tensor that was unused. Tensor will be ignored. (index=4, shape=torch.Size([1, 3]), dtype=torch.float32)\n",
      "  warnings.warn(\n",
      "/opt/aws_neuronx_venv_pytorch_2_6_nxd_inference/lib/python3.10/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:210: UserWarning: Received an input tensor that was unused. Tensor will be ignored. (index=5, shape=torch.Size([1]), dtype=torch.int32)\n",
      "  warnings.warn(\n",
      "/opt/aws_neuronx_venv_pytorch_2_6_nxd_inference/lib/python3.10/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:210: UserWarning: Received an input tensor that was unused. Tensor will be ignored. (index=6, shape=torch.Size([1]), dtype=torch.int32)\n",
      "  warnings.warn(\n",
      "INFO:Neuron:Finished generating HLO for context_encoding_model in 2.9438202381134033 seconds, input example shape = torch.Size([1, 16])\n",
      "INFO:Neuron:Generating 1 hlos for key: token_generation_model\n",
      "INFO:Neuron:Started loading module token_generation_model\n",
      "INFO:Neuron:Finished loading module token_generation_model in 0.07202529907226562 seconds\n",
      "INFO:Neuron:generating HLO: token_generation_model, input example shape = torch.Size([1, 1])\n",
      "INFO:Neuron:Finished generating HLO for token_generation_model in 2.7211008071899414 seconds, input example shape = torch.Size([1, 1])\n",
      "INFO:Neuron:Generated all HLOs in 5.901822566986084 seconds\n",
      "INFO:Neuron:Starting compilation for the priority HLO\n",
      "INFO:Neuron:'token_generation_model' is the priority model with bucket rank 0\n",
      "/opt/aws_neuronx_venv_pytorch_2_6_nxd_inference/lib/python3.10/site-packages/libneuronxla/neuron_cc_wrapper.py:283: SyntaxWarning: str format compiler_flags is discouraged as its handling involves repeated joining and splitting, which can easily make mistakes if something is quoted or escaped. Use list[str] instead. Refer to documentation of the Python subprocess module for details.\n",
      "  warnings.warn(SyntaxWarning(\n",
      "2025-06-02 14:42:31.000195:  14830  INFO ||NEURON_CC_WRAPPER||: Call compiler with cmd: neuronx-cc compile --framework=XLA /tmp/nxd_model/token_generation_model/_tp0_bk0/model.MODULE_567529bf12f9decb7698+91ef39e9.hlo_module.pb --output /tmp/nxd_model/token_generation_model/_tp0_bk0/model.MODULE_567529bf12f9decb7698+91ef39e9.neff --target=trn1 --auto-cast=none --model-type=transformer --tensorizer-options=--enable-ccop-compute-overlap --cc-pipeline-tiling-factor=2 --vectorize-strided-dma  --lnc=1 --logfile=/tmp/nxd_model/token_generation_model/_tp0_bk0/log-neuron-cc.txt --enable-internal-neff-wrapper --verbose=35\n",
      "....Completed run_backend_driver.\n",
      "\n",
      "Compiler status PASS\n",
      "INFO:Neuron:Done compilation for the priority HLO in 79.0590603351593 seconds\n",
      "INFO:Neuron:Updating the hlo module with optimized layout\n",
      "INFO:Neuron:Done optimizing weight layout for all HLOs in 0.1603398323059082 seconds\n",
      "INFO:Neuron:Starting compilation for all HLOs\n",
      "/opt/aws_neuronx_venv_pytorch_2_6_nxd_inference/lib/python3.10/site-packages/libneuronxla/neuron_cc_wrapper.py:245: SyntaxWarning: str format compiler_flags is discouraged as its handling involves repeated joining and splitting, which can easily make mistakes if something is quoted or escaped. Use list[str] instead. Refer to documentation of the Python subprocess module for details.\n",
      "  warnings.warn(SyntaxWarning(\n",
      "2025-06-02 14:43:50.000415:  14830  INFO ||NEURON_CC_WRAPPER||: Call compiler with cmd: neuronx-cc compile --framework=XLA /tmp/nxd_model/context_encoding_model/_tp0_bk0/model.MODULE_1a1c1c2d2921cf269654+d43b5474.hlo_module.pb --output /tmp/nxd_model/context_encoding_model/_tp0_bk0/model.MODULE_1a1c1c2d2921cf269654+d43b5474.neff --target=trn1 --auto-cast=none --model-type=transformer --tensorizer-options=--enable-ccop-compute-overlap --cc-pipeline-tiling-factor=2 --vectorize-strided-dma  --lnc=1 -O1 --internal-hlo2tensorizer-options= --modular-flow-mac-threshold=10  --logfile=/tmp/nxd_model/context_encoding_model/_tp0_bk0/log-neuron-cc.txt --verbose=35\n",
      ".Completed run_backend_driver.\n",
      "\n",
      "Compiler status PASS\n",
      "INFO:Neuron:Finished Compilation for all HLOs in 8.358030796051025 seconds\n",
      "..Completed run_backend_driver.\n",
      "\n",
      "Compiler status PASS\n",
      "INFO:Neuron:Done preparing weight layout transformation\n",
      "INFO:Neuron:Finished building model in 127.03067421913147 seconds\n",
      "INFO:Neuron:SKIPPING pre-sharding the checkpoints. The checkpoints will be sharded during load time.\n",
      "Compiling and tracing time: 127.04125871100041 seconds\n",
      "\n",
      "Loading model to Neuron...\n",
      "INFO:Neuron:Sharding weights on load...\n",
      "INFO:Neuron:Sharding Weights for ranks: 0...7\n",
      "[2025-06-02 14:44:32.211: I neuronx_distributed/parallel_layers/parallel_state.py:592] > initializing tensor model parallel with size 8\n",
      "[2025-06-02 14:44:32.211: I neuronx_distributed/parallel_layers/parallel_state.py:593] > initializing pipeline model parallel with size 1\n",
      "[2025-06-02 14:44:32.211: I neuronx_distributed/parallel_layers/parallel_state.py:594] > initializing context model parallel with size 1\n",
      "[2025-06-02 14:44:32.212: I neuronx_distributed/parallel_layers/parallel_state.py:595] > initializing data parallel with size 1\n",
      "[2025-06-02 14:44:32.212: I neuronx_distributed/parallel_layers/parallel_state.py:596] > initializing world size to 8\n",
      "[2025-06-02 14:44:32.213: I neuronx_distributed/parallel_layers/parallel_state.py:343] [rank_0_pp-1_tp-1_dp-1_cp-1] Chosen Logic for replica groups ret_logic=<PG_Group_Logic.LOGIC1: (<function ascending_ring_PG_group at 0x7ca44a1b7a30>, 'Ascending Ring PG Group')>\n",
      "[2025-06-02 14:44:32.213: I neuronx_distributed/parallel_layers/parallel_state.py:632] [rank_0_pp-1_tp-1_dp-1_cp-1] tp_groups: replica_groups.tp_groups=[[0, 1, 2, 3, 4, 5, 6, 7]]\n",
      "[2025-06-02 14:44:32.213: I neuronx_distributed/parallel_layers/parallel_state.py:633] [rank_0_pp-1_tp-1_dp-1_cp-1] dp_groups: replica_groups.dp_groups=[[0], [1], [2], [3], [4], [5], [6], [7]]\n",
      "[2025-06-02 14:44:32.213: I neuronx_distributed/parallel_layers/parallel_state.py:634] [rank_0_pp-1_tp-1_dp-1_cp-1] pp_groups: replica_groups.pp_groups=[[0], [1], [2], [3], [4], [5], [6], [7]]\n",
      "[2025-06-02 14:44:32.213: I neuronx_distributed/parallel_layers/parallel_state.py:635] [rank_0_pp-1_tp-1_dp-1_cp-1] cp_groups: replica_groups.cp_groups=[[0], [1], [2], [3], [4], [5], [6], [7]]\n",
      "[2025-06-02 14:44:32.213: I neuronx_distributed/parallel_layers/parallel_state.py:636] [rank_0_pp-1_tp-1_dp-1_cp-1] ep_model_groups: replica_groups.ep_model_groups=[[0], [1], [2], [3], [4], [5], [6], [7]]\n",
      "[2025-06-02 14:44:32.213: I neuronx_distributed/parallel_layers/parallel_state.py:637] [rank_0_pp-1_tp-1_dp-1_cp-1] ep_data_groups: replica_groups.ep_data_groups=[[0], [1], [2], [3], [4], [5], [6], [7]]\n",
      "INFO:Neuron:Done Sharding weights in 1.0740620010001294\n",
      "INFO:Neuron:Finished weights loading in 11.170274965999852 seconds\n",
      "INFO:Neuron:Warming up the model.\n",
      "2025-Jun-02 14:44:43.0885 14830:15971 [0] nccl_net_ofi_create_plugin:211 CCOM WARN NET/OFI Failed to initialize sendrecv protocol\n",
      "2025-Jun-02 14:44:43.0887 14830:15971 [0] nccl_net_ofi_create_plugin:334 CCOM WARN NET/OFI aws-ofi-nccl initialization failed\n",
      "2025-Jun-02 14:44:43.0888 14830:15971 [0] nccl_net_ofi_init:155 CCOM WARN NET/OFI Initializing plugin failed\n",
      "2025-Jun-02 14:44:43.0890 14830:15971 [0] net_plugin.cc:94 CCOM WARN OFI plugin initNet() failed is EFA enabled?\n",
      "INFO:Neuron:Warmup completed in 0.2749016284942627 seconds.\n",
      "Total model loading time: 11.964653464000548 seconds\n",
      "/opt/aws_neuronx_venv_pytorch_2_6_nxd_inference/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:653: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "\n",
      "Checking accuracy by logit matching\n",
      "/opt/aws_neuronx_venv_pytorch_2_6_nxd_inference/lib/python3.10/site-packages/neuronx_distributed_inference/utils/accuracy.py:363: UserWarning: input_len + num_tokens_to_check exceeds max_context_length. If output divergences at an index greater than max_context_length, a ValueError will occur because the next input len exceeds max_context_length. To avoid this, set num_tokens_to_check to a value of max_context_length - input_len or less.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  2.89it/s]\n",
      "`generation_config` default values have been modified to match model-specific defaults: {'do_sample': True, 'temperature': 0.6, 'top_p': 0.95}. If this is not desired, please set these values explicitly.\n",
      "/opt/aws_neuronx_venv_pytorch_2_6_nxd_inference/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/opt/aws_neuronx_venv_pytorch_2_6_nxd_inference/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Expected Output:  [\", that is the question. Whether 'tis nobler in the mind to suffer the slings and arrows of outrageous fortune\"] tensor([[   11,   429,   374,   279,  3405,    13, 13139,   364,    83,   285,\n",
      "         13049,  1536,   304,   279,  3971,   311,  7676,   279,  1739,   819,\n",
      "           323, 36957,   315, 54488, 32315]])\n",
      "Expected Logits Shape:  torch.Size([25, 1, 151936])\n",
      "HuggingFaceGenerationAdapter has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "Actual Output:  [\", that is the question. Whether 'tis nobler in the mind to suffer the slings and arrows of outrageous fortune\"] tensor([[   11,   429,   374,   279,  3405,    13, 13139,   364,    83,   285,\n",
      "         13049,  1536,   304,   279,  3971,   311,  7676,   279,  1739,   819,\n",
      "           323, 36957,   315, 54488, 32315]])\n",
      "Actual Logits Shape:  torch.Size([25, 1, 151936])\n",
      "Passed logits validation!\n",
      "\n",
      "Generating outputs...\n",
      "Prompts: ['To be, or not to be']\n",
      "Generated outputs:\n",
      "Output 0: To be, or not to be, that is the question. Whether 'tis nobler in the mind to suffer the slings and arrows of outrageous fortune\n",
      "Starting end-to-end benchmark with 20\n",
      "Benchmark completed and its result is as following\n",
      "{\n",
      "    \"e2e_model\": {\n",
      "        \"latency_ms_p50\": 169.31116580963135,\n",
      "        \"latency_ms_p90\": 172.9245901107788,\n",
      "        \"latency_ms_p95\": 174.3390679359436,\n",
      "        \"latency_ms_p99\": 174.82486009597778,\n",
      "        \"latency_ms_p100\": 174.94630813598633,\n",
      "        \"latency_ms_avg\": 169.6009874343872,\n",
      "        \"throughput\": 188.67814677305284\n",
      "    },\n",
      "    \"context_encoding_model\": {\n",
      "        \"latency_ms_p50\": 13.715386390686035,\n",
      "        \"latency_ms_p90\": 13.958406448364258,\n",
      "        \"latency_ms_p95\": 13.969480991363525,\n",
      "        \"latency_ms_p99\": 13.981258869171143,\n",
      "        \"latency_ms_p100\": 13.984203338623047,\n",
      "        \"latency_ms_avg\": 13.787257671356201,\n",
      "        \"throughput\": 1160.4918382892702\n",
      "    },\n",
      "    \"token_generation_model\": {\n",
      "        \"latency_ms_p50\": 8.931398391723633,\n",
      "        \"latency_ms_p90\": 9.162139892578125,\n",
      "        \"latency_ms_p95\": 9.23851728439331,\n",
      "        \"latency_ms_p99\": 9.780135154724094,\n",
      "        \"latency_ms_p100\": 12.94398307800293,\n",
      "        \"latency_ms_avg\": 9.013524055480957,\n",
      "        \"throughput\": 118.34069117705926\n",
      "    }\n",
      "}\n",
      "Completed saving result to benchmark_report.json\n"
     ]
    }
   ],
   "source": [
    "!inference_demo \\\n",
    "    --model-type qwen3 \\\n",
    "    --task-type causal-lm \\\n",
    "    run \\\n",
    "    --model-path /home/ubuntu/model_hf_qwen/qwen/ \\\n",
    "    --compiled-model-path /home/ubuntu/traced_model_qwen/qwen/logit \\\n",
    "    --torch-dtype bfloat16 \\\n",
    "    --tp-degree 8 \\\n",
    "    --batch-size 1 \\\n",
    "    --max-context-length 16 \\\n",
    "    --seq-len 32 \\\n",
    "    --enable-bucketing \\\n",
    "    --pad-token-id 151645 \\\n",
    "    --prompt \"To be, or not to be\" \\\n",
    "    --check-accuracy-mode logit-matching \\\n",
    "    --benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aws_neuronx_venv_pytorch_2_6_nxd_inference",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
